{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from pytorch_transformers import RobertaTokenizer\n",
    "\n",
    "from src.eqt.datasets import create_test_dataset_for_prediction\n",
    "from src.eqt.preprocess_data import preprocess_data\n",
    "from src.eqt.model_qbert import QBERT\n",
    "\n",
    "\n",
    "def get_label_source(row, class_tag):\n",
    "    if row[class_tag] != '':\n",
    "        return row['label_source']\n",
    "    return 'QBERT'\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    'type': {\n",
    "        'Ask about antecedent':  0,\n",
    "        'Ask about consequence': 1,\n",
    "        'Ask for confirmation':  2,\n",
    "        'Irony':                 3,\n",
    "        'Negative rhetoric':     4,\n",
    "        'Positive rhetoric':     5,\n",
    "        'Request information':   6,\n",
    "        'Suggest a reason':      7,\n",
    "        'Suggest a solution':    8,\n",
    "    },\n",
    "    'intent': {\n",
    "        'Amplify excitement': 0,\n",
    "        'Amplify joy':        1,\n",
    "        'Amplify pride':      2,\n",
    "        'De-escalate':        3,\n",
    "        'Express concern':    4,\n",
    "        'Express interest':   5,\n",
    "        'Moralize speaker':   6,\n",
    "        'Motivate':           7,\n",
    "        'Offer relief':       8,\n",
    "        'Pass judgement':     9,\n",
    "        'Support':           10,\n",
    "        'Sympathize':        11,\n",
    "    }\n",
    "}\n",
    "\n",
    "class_type = \"type\"\n",
    "peak_lr = 2e-5\n",
    "checkpoints_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RobertaTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m layer_norm_eps     \u001b[39m=\u001b[39m \u001b[39m1e-5\u001b[39m\n\u001b[1;32m      9\u001b[0m max_position_embed \u001b[39m=\u001b[39m \u001b[39m514\u001b[39m\n\u001b[0;32m---> 11\u001b[0m tokenizer  \u001b[39m=\u001b[39m RobertaTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mroberta-base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m vocab_size \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m     14\u001b[0m adam_beta_1  \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RobertaTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "num_layers         = 12\n",
    "d_model            = 768\n",
    "num_heads          = 12\n",
    "dff                = d_model * 4\n",
    "hidden_act         = \"gelu\"\n",
    "dropout_rate       = 0.1\n",
    "layer_norm_eps     = 1e-5\n",
    "max_position_embed = 514\n",
    "\n",
    "tokenizer  = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "lab_mapping  = mappings[class_type]\n",
    "pred_mapping = {v: k for k, v in lab_mapping.items()}\n",
    "num_classes  = len(pred_mapping.keys())\n",
    "\n",
    "adam_beta_1  = 0.9\n",
    "adam_beta_2  = 0.98\n",
    "adam_epsilon = 1e-6\n",
    "\n",
    "qbert = QBERT(num_layers, \n",
    "              d_model, \n",
    "              num_heads, \n",
    "              dff, \n",
    "              hidden_act, \n",
    "              dropout_rate,\n",
    "              layer_norm_eps, \n",
    "              max_position_embed, \n",
    "              vocab_size, \n",
    "              num_classes)\n",
    "optimizer = tf.keras.optimizers.Adam(peak_lr, \n",
    "                                     beta_1=adam_beta_1, \n",
    "                                     beta_2=adam_beta_2,\n",
    "                                     epsilon=adam_epsilon)\n",
    "ckpt = tf.train.Checkpoint(model=qbert, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoints_path, max_to_keep=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample-param",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
